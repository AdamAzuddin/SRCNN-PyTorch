{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_y_channel(image):\n",
    "    ycbcr = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    y = ycbcr[:, :, 0]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    channels, height, width = image.size()\n",
    "    patch_channels, patch_height, patch_width = patch_size\n",
    "    patches = []\n",
    "\n",
    "    # Pad the image to ensure its dimensions are divisible by the patch size\n",
    "    pad_height = patch_height - height % patch_height\n",
    "    pad_width = patch_width - width % patch_width\n",
    "    padded_image = F.pad(image, (0, pad_width, 0, pad_height), mode='constant', value=0)\n",
    "\n",
    "    # Extract patches from the padded image\n",
    "    for y in range(0, height, patch_height):\n",
    "        for x in range(0, width, patch_width):\n",
    "            patch = padded_image[:, y:y+patch_height, x:x+patch_width]\n",
    "            patches.append(patch)\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_folder(folder_path, scale, patch_size):\n",
    "    processed_images = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        input_image = cv2.imread(img_path)\n",
    "\n",
    "        GT_image_y = convert_to_y_channel(input_image)\n",
    "\n",
    "        GT_tensor = torch.tensor(GT_image_y, dtype=torch.float32).unsqueeze(0) / 255.0  # Add channel dimension and normalize\n",
    "\n",
    "        # Interpolate GT image to create LR image\n",
    "        GT_image_y_pil = Image.fromarray(GT_image_y)  # Convert Y channel to PIL image for resizing\n",
    "        LR_image = TF.resize(GT_image_y_pil, [GT_image_y_pil.size[1] // scale, GT_image_y_pil.size[0] // scale], interpolation=Image.BICUBIC)\n",
    "\n",
    "        # Resize LR image to match the size of GT image\n",
    "        LR_image_resized = TF.resize(LR_image, [GT_tensor.shape[1], GT_tensor.shape[2]], interpolation=Image.BICUBIC)\n",
    "        LR_tensor_resized = TF.to_tensor(LR_image_resized).float()\n",
    "\n",
    "        LR_patches = image_to_patches(LR_tensor_resized, patch_size)\n",
    "        GT_patches = image_to_patches(GT_tensor, patch_size)\n",
    "\n",
    "        # Append LR and GT patches as a tuple\n",
    "        processed_images.extend(zip(LR_patches, GT_patches))\n",
    "\n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(LR, GT):\n",
    "    \n",
    "    # Display the label image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(GT.permute(1, 2, 0))\n",
    "    plt.title('Ground Truth (GT)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(LR.permute(1, 2, 0))\n",
    "    plt.title('Low Resolution (LR)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing training and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_folder_path = 'data/Train/ILSVRC2013'\n",
    "test_set5_folder_path = 'data/Test/Set5/'\n",
    "test_set14_folder_path = 'data/Test/Set14/'\n",
    "scale = 3\n",
    "\n",
    "processed_train_images = preprocess_images_in_folder(train_folder_path, scale, (1, 33,33))\n",
    "processed_test_set5_images = preprocess_images_in_folder(test_set5_folder_path, scale, (1, 33,33))\n",
    "processed_test_set14_images = preprocess_images_in_folder(test_set14_folder_path, scale, (1, 33, 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display random test images from Set5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = random.choice(processed_test_set5_images)\n",
    "LR, GT = random_image\n",
    "\n",
    "display_image(LR, GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting preprocessed images in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(processed_train_images, batch_size=128)\n",
    "test_set5_loader = DataLoader(processed_test_set5_images, batch_size=128)\n",
    "test_set14_loader = DataLoader(processed_test_set14_images, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining SRCNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, stride=1, padding=4)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.conv1(x))\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRCNN()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0003)\n",
    "\n",
    "num_epochs = 100\n",
    "best_model_wts = None\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop \n",
    "- Training and validation loss is printed every 10 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for LR, GT in train_loader:\n",
    "        LR, GT = LR.to(device), GT.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(LR)\n",
    "        \n",
    "        loss = criterion(outputs, GT)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * LR.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss}')\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for LR, GT in test_set5_loader:\n",
    "            LR, GT = LR.to(device), GT.to(device)\n",
    "            outputs = model(LR)\n",
    "            loss = criterion(outputs, GT)\n",
    "            val_loss += loss.item() * LR.size(0)\n",
    "    \n",
    "    val_loss /= len(test_set5_loader.dataset)\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss Set 5: {val_loss}')\n",
    "    \n",
    "    # Save the best model weights\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = model.state_dict()\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'super_resolution_model_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set 5\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (val_inputs, val_labels) in enumerate(test_set5_loader):\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        \n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = val_loss / len(test_set5_loader)\n",
    "    print(f\"Validation Loss for Set 5: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set 14\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (val_inputs, val_labels) in enumerate(test_set14_loader):\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        \n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = val_loss / len(test_set14_loader)\n",
    "    print(f\"Validation Loss for Set 14: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving inferenced images from test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Cb and Cr value of an image for reconversion ro RGB format later\n",
    "def extract_cb_cr(image):\n",
    "    ycbcr = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    cb = ycbcr[:, :, 1]\n",
    "    cr = ycbcr[:, :, 2]\n",
    "    return cb, cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_image(input_image_path, output_image_path):\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "\n",
    "    input_image_Cb, input_image_Cr = extract_cb_cr(input_image)\n",
    "    \n",
    "    input_image_Y = convert_to_y_channel(input_image)\n",
    "\n",
    "    input_tensor = torch.tensor(input_image_Y, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0  # Add channel and batch dimensions and normalize\n",
    "\n",
    "    # Process image for GT (ground truth) and LR (low resolution)\n",
    "    GT_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        SR_tensor = model(GT_tensor)\n",
    "\n",
    "    # Post-process SR tensor\n",
    "    SR_tensor = SR_tensor.squeeze().cpu().numpy()  # Remove batch and channel dimensions and convert to NumPy\n",
    "    SR_tensor = np.clip(SR_tensor * 255.0, 0, 255).astype(np.uint8)  # Rescale to 0-255 and convert to uint8\n",
    "\n",
    "    # Combine SR Y channel with original Cb and Cr channels\n",
    "    SR_ycbcr = cv2.merge((SR_tensor, input_image_Cb, input_image_Cr))\n",
    "    SR_image = cv2.cvtColor(SR_ycbcr, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    # Save the super-resolved image\n",
    "    cv2.imwrite(output_image_path, SR_image)\n",
    "\n",
    "# Directories\n",
    "input_dirs = ['data/Test/Set5', 'data/Test/Set14']\n",
    "output_dir = 'results'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all images in the specified directories\n",
    "image_paths = []\n",
    "for input_dir in input_dirs:\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.bmp'):\n",
    "            input_image_path = os.path.join(input_dir, filename)\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            output_image_name = f\"{base_name.split('_')[0]}_SR{ext}\"\n",
    "            output_image_path = os.path.join(output_dir, output_image_name)\n",
    "\n",
    "            process_and_save_image(input_image_path, output_image_path)\n",
    "            image_paths.append((input_image_path, output_image_path))\n",
    "            print(f\"Processed and saved: {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input_image_path, random_output_image_path = random.choice(image_paths)\n",
    "\n",
    "# Load the original and super-resolved images\n",
    "original_image = Image.open(random_input_image_path).convert('RGB')  # Load original image\n",
    "super_resolved_image = Image.open(random_output_image_path).convert('RGB')  # Load super-resolved image\n",
    "\n",
    "# Display the images side by side using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Super-resolved image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(super_resolved_image)\n",
    "plt.title('Super-resolved Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
