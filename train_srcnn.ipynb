{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of Image Super-Resolution Using Deep Convolutional Network in PyTorch\n",
    "\n",
    "This notebook is the reimplementation of this [paper](https://arxiv.org/abs/1501.00092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.input = self.file['data']\n",
    "        self.target = self.file['label']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        input = torch.tensor(self.input[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        \n",
    "        return input, target\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MyDataset\n",
    "dataset = MyDataset('train.h5')\n",
    "test_dataset = MyDataset('test.h5')\n",
    "\n",
    "# Choose the index of the data point you want to print\n",
    "index_to_print = 4\n",
    "\n",
    "# Retrieve the specified data point from the dataset\n",
    "input, target = dataset[index_to_print]\n",
    "image_test, label_test = test_dataset[index_to_print]\n",
    "\n",
    "\n",
    "print(\"Image input size: \", input.size())\n",
    "\n",
    "\n",
    "# Convert the image tensor to a PIL image for visualization\n",
    "pil_input = to_pil_image(input)\n",
    "pil_target = to_pil_image(target)\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "plt.imshow(pil_input)\n",
    "plt.title(\"Input image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Image target size: \", target.size())\n",
    "# Print information about the label\n",
    "plt.title(\"Target image\")\n",
    "plt.imshow(pil_target)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Close the dataset (important for releasing resources)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Datasets and Dataloaders\n",
    "\n",
    "The train.h5 and test.h5 files are obtained by running the MATLAB scripts generate_train and generate_test respectively from [this](https://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_train.zip) source code given in the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and dataloader\n",
    "dataset = MyDataset('train.h5') \n",
    "train_loader = DataLoader(dataset, batch_size=128)\n",
    "\n",
    "dataset_test_set5 = MyDataset('test.h5')\n",
    "val_loader = DataLoader(dataset_test_set5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SRCNN model\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Feature extraction layer\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4),  # Input channels: 3 (RGB)\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Non-linear mapping layer\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Reconstruction layer\n",
    "        self.reconstruction = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=1.5715, mode='bicubic')  # Upsample to match target size (33x33)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.map(out)\n",
    "        out = self.reconstruction(out)\n",
    "        out = self.upsample(out)\n",
    "        return out\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights with Gaussian distribution (std=0.001) and bias with zeros\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Initialize convolutional layer weights with Gaussian distribution (std=0.001)\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    # Initialize bias with zeros\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4),  # Input channels: 3 (RGB)\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.reconstruction = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.map(out)\n",
    "        out = self.reconstruction(out)\n",
    "        return out[:, :, 6:-6, 6:-6]  # Crop the output to match the size 21x21\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SRCNN()\n",
    "criterion = nn.MSELoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_baseline = end_time - start_time\n",
    "print(f\"Training completed in {training_time_baseline:.2f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'srcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "to_pil_image = ToPILImage()\n",
    "\n",
    "# Evaluate model on validation dataset\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (val_inputs, val_labels) in enumerate(val_loader):\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        \n",
    "        # Ensure the output and target sizes match\n",
    "        if val_outputs.size() != val_labels.size():\n",
    "            val_outputs = F.interpolate(val_outputs, size=val_labels.size()[2:], mode='bicubic', align_corners=False)\n",
    "        \n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {average_val_loss}\")\n",
    "\n",
    "    # Visualize some example results\n",
    "    index_to_print = 23\n",
    "    val_inputs, val_labels = next(iter(val_loader))\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "    val_outputs = model(val_inputs)\n",
    "    \n",
    "    if val_outputs.size() != val_labels.size():\n",
    "        val_outputs = F.interpolate(val_outputs, size=val_labels.size()[2:], mode='bicubic', align_corners=False)\n",
    "    \n",
    "    val_outputs = val_outputs.cpu()\n",
    "    val_labels = val_labels.cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(to_pil_image(val_labels[index_to_print]))\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(to_pil_image(val_outputs[index_to_print]))\n",
    "    plt.title('Model Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = 'butterfly_GT.bmp'\n",
    "input_image = Image.open(input_image_path).convert('RGB')\n",
    "\n",
    "# Define patch size and stride\n",
    "input_patch_size = 33\n",
    "output_patch_size = 21\n",
    "stride = 14\n",
    "stride = 10\n",
    "\n",
    "# Extract patches from the input image\n",
    "patches = []\n",
    "image_width, image_height = input_image.size\n",
    "\n",
    "for y in range(0, image_height - input_patch_size + 1, stride):\n",
    "    for x in range(0, image_width - input_patch_size + 1, stride):\n",
    "        # Crop input patch from the image\n",
    "        input_patch = input_image.crop((x, y, x + input_patch_size, y + input_patch_size))\n",
    "        input_patch_tensor = TF.to_tensor(input_patch)  # Convert patch to tensor (RGB format)\n",
    "        patches.append(input_patch_tensor)\n",
    "\n",
    "# Convert list of tensors to a batch tensor\n",
    "input_batch = torch.stack(patches)\n",
    "\n",
    "# Initialize and load the trained SRCNN model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load('srcnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Perform inference on the input patches using the model\n",
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "# Visualize and evaluate a specific output patch\n",
    "patch_index = 21  # Example patch index to visualize\n",
    "\n",
    "# Convert output patch tensor to a PIL image\n",
    "output_patch_tensor = output_batch[patch_index].detach().cpu()\n",
    "output_patch_pil = TF.to_pil_image(output_patch_tensor)\n",
    "\n",
    "# Display the input patch and the model's output (super-resolved patch)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Display input patch\n",
    "plt.subplot(1, 2, 1)\n",
    "input_patch_pil = TF.to_pil_image(input_batch[patch_index].detach().cpu())\n",
    "plt.imshow(input_patch_pil)\n",
    "plt.title('Input Patch (33x33)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display output patch (super-resolved)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_patch_pil)\n",
    "plt.title('Super-Resolved Patch (21x21)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the images using test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = \"zebra.bmp\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define patch size and stride\n",
    "input_patch_size = 33\n",
    "output_patch_size = 21\n",
    "stride = 14\n",
    "\n",
    "# Extract patches from the input image\n",
    "patches = []\n",
    "coords = []\n",
    "image_width, image_height = input_image.size\n",
    "\n",
    "for y in range(0, image_height - input_patch_size + 1, stride):\n",
    "    for x in range(0, image_width - input_patch_size + 1, stride):\n",
    "        # Crop input patch from the image\n",
    "        input_patch = input_image.crop(\n",
    "            (x, y, x + input_patch_size, y + input_patch_size)\n",
    "        )\n",
    "        input_patch_tensor = TF.to_tensor(\n",
    "            input_patch\n",
    "        )  # Convert patch to tensor (RGB format)\n",
    "        patches.append(input_patch_tensor)\n",
    "        coords.append((x, y))\n",
    "\n",
    "# Convert list of tensors to a batch tensor\n",
    "input_batch = torch.stack(patches)\n",
    "\n",
    "# Initialize and load the trained SRCNN model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load(\"srcnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Perform inference on the input patches using the model\n",
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "# Initialize an empty tensor for the output image\n",
    "output_image = torch.zeros(3, image_height, image_width)\n",
    "\n",
    "# Initialize an empty tensor to count overlapping regions\n",
    "overlap_count = torch.zeros(image_height, image_width)\n",
    "\n",
    "# Place each output patch back into the correct location in the output image\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    output_patch = output_batch[i]\n",
    "    output_image[:, y : y + output_patch_size, x : x + output_patch_size] += output_patch\n",
    "    overlap_count[y : y + output_patch_size, x : x + output_patch_size] += 1\n",
    "\n",
    "# Average overlapping regions\n",
    "output_image /= overlap_count\n",
    "\n",
    "# Convert the output image tensor to a PIL image\n",
    "output_image_pil = TF.to_pil_image(output_image.clamp(0, 1))\n",
    "\n",
    "\n",
    "# Display the input image and the super-resolved image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Display input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Display super-resolved image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_image_pil)\n",
    "plt.title(\"Super-Resolved Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = \"baby_GT.bmp\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define patch size and stride\n",
    "input_patch_size = 33\n",
    "output_patch_size = 21\n",
    "stride = 14\n",
    "\n",
    "# Extract patches from the input image\n",
    "patches = []\n",
    "coords = []\n",
    "image_width, image_height = input_image.size\n",
    "\n",
    "for y in range(0, image_height - input_patch_size + 1, stride):\n",
    "    for x in range(0, image_width - input_patch_size + 1, stride):\n",
    "        # Crop input patch from the image\n",
    "        input_patch = input_image.crop(\n",
    "            (x, y, x + input_patch_size, y + input_patch_size)\n",
    "        )\n",
    "        input_patch_tensor = TF.to_tensor(\n",
    "            input_patch\n",
    "        )  # Convert patch to tensor (RGB format)\n",
    "        patches.append(input_patch_tensor)\n",
    "        coords.append((x, y))\n",
    "\n",
    "# Convert list of tensors to a batch tensor\n",
    "input_batch = torch.stack(patches)\n",
    "\n",
    "# Initialize and load the trained SRCNN model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load(\"srcnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Perform inference on the input patches using the model\n",
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "# Initialize an empty tensor for the output image\n",
    "output_image = torch.zeros(3, image_height, image_width)\n",
    "\n",
    "# Initialize an empty tensor to count overlapping regions\n",
    "overlap_count = torch.zeros(image_height, image_width)\n",
    "\n",
    "# Place each output patch back into the correct location in the output image\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    output_patch = output_batch[i]\n",
    "    output_image[:, y : y + output_patch_size, x : x + output_patch_size] += output_patch\n",
    "    overlap_count[y : y + output_patch_size, x : x + output_patch_size] += 1\n",
    "\n",
    "# Average overlapping regions\n",
    "output_image /= overlap_count\n",
    "\n",
    "# Convert the output image tensor to a PIL image\n",
    "output_image_pil = TF.to_pil_image(output_image.clamp(0, 1))\n",
    "\n",
    "# Display the input image and the super-resolved image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Display input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Display super-resolved image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_image_pil)\n",
    "plt.title(\"Super-Resolved Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = \"baby_GT.bmp\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define patch size and stride\n",
    "input_patch_size = 33\n",
    "output_patch_size = 21\n",
    "stride = 14\n",
    "\n",
    "# Extract patches from the input image\n",
    "patches = []\n",
    "coords = []\n",
    "image_width, image_height = input_image.size\n",
    "\n",
    "for y in range(0, image_height - input_patch_size + 1, stride):\n",
    "    for x in range(0, image_width - input_patch_size + 1, stride):\n",
    "        # Crop input patch from the image\n",
    "        input_patch = input_image.crop(\n",
    "            (x, y, x + input_patch_size, y + input_patch_size)\n",
    "        )\n",
    "        input_patch_tensor = TF.to_tensor(\n",
    "            input_patch\n",
    "        )  # Convert patch to tensor (RGB format)\n",
    "        patches.append(input_patch_tensor)\n",
    "        coords.append((x, y))\n",
    "\n",
    "# Convert list of tensors to a batch tensor\n",
    "input_batch = torch.stack(patches)\n",
    "\n",
    "# Initialize and load the trained SRCNN model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load(\"srcnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Perform inference on the input patches using the model\n",
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "# Initialize an empty tensor for the output image\n",
    "output_image = torch.zeros(3, image_height, image_width)\n",
    "\n",
    "# Place each output patch back into the correct location in the output image\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    output_patch = output_batch[i]\n",
    "    output_image[:, y : y + output_patch_size, x : x + output_patch_size] += output_patch\n",
    "\n",
    "# Average overlapping regions\n",
    "overlap_count = torch.zeros(image_height, image_width)\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    overlap_count[y : y + output_patch_size, x : x + output_patch_size] += 1\n",
    "\n",
    "output_image /= overlap_count\n",
    "\n",
    "# Convert the output image tensor to a PIL image\n",
    "output_image_pil = TF.to_pil_image(output_image.clamp(0, 1))\n",
    "\n",
    "# Display the input image and the super-resolved image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Display input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Display super-resolved image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_image_pil)\n",
    "plt.title(\"Super-Resolved Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "input_image_path = \"butterfly_GT.bmp\"\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define patch size and stride\n",
    "input_patch_size = 33\n",
    "output_patch_size = 21\n",
    "stride = 14\n",
    "\n",
    "# Extract patches from the input image\n",
    "patches = []\n",
    "coords = []\n",
    "image_width, image_height = input_image.size\n",
    "\n",
    "for y in range(0, image_height - input_patch_size + 1, stride):\n",
    "    for x in range(0, image_width - input_patch_size + 1, stride):\n",
    "        # Crop input patch from the image\n",
    "        input_patch = input_image.crop(\n",
    "            (x, y, x + input_patch_size, y + input_patch_size)\n",
    "        )\n",
    "        input_patch_tensor = TF.to_tensor(\n",
    "            input_patch\n",
    "        )  # Convert patch to tensor (RGB format)\n",
    "        patches.append(input_patch_tensor)\n",
    "        coords.append((x, y))\n",
    "\n",
    "# Convert list of tensors to a batch tensor\n",
    "input_batch = torch.stack(patches)\n",
    "\n",
    "# Initialize and load the trained SRCNN model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load(\"srcnn_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Perform inference on the input patches using the model\n",
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "# Initialize an empty tensor for the output image\n",
    "output_image = torch.zeros(3, image_height, image_width)\n",
    "\n",
    "# Place each output patch back into the correct location in the output image\n",
    "# Place each output patch back into the correct location in the output image\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    output_patch = output_batch[i]\n",
    "    output_image[:, y : y + output_patch_size, x : x + output_patch_size] = (\n",
    "        output_patch[\n",
    "            :,\n",
    "            (input_patch_size - output_patch_size)\n",
    "            // 2 : (input_patch_size + output_patch_size)\n",
    "            // 2,\n",
    "            (input_patch_size - output_patch_size)\n",
    "            // 2 : (input_patch_size + output_patch_size)\n",
    "            // 2,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert the output image tensor to a PIL image\n",
    "output_image_pil = TF.to_pil_image(output_image.clamp(0, 1))\n",
    "\n",
    "\n",
    "# Display the input image and the super-resolved image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Display input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Display super-resolved image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_image_pil)\n",
    "plt.title(\"Super-Resolved Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PSNR values for validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate PSNR between two images\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')  # PSNR is infinite if images are identical\n",
    "    MAX = 255.0\n",
    "    psnr = 10 * np.log10((MAX**2) / mse)\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store PSNR values\n",
    "psnr_values = []\n",
    "\n",
    "# Iterate over validation loader\n",
    "for inputs, labels in val_loader:\n",
    "    # Move inputs and labels to the device\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Generate predictions using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "    \n",
    "    # Convert tensors to numpy arrays (RGB images)\n",
    "    outputs = outputs.cpu().detach().numpy()  # Convert model outputs to numpy arrays\n",
    "    print(outputs[0])\n",
    "    labels = labels.cpu().detach().numpy()  # Convert labels to numpy arrays\n",
    "    \n",
    "    output = to_pil_image(outputs.squeeze().cpu().numpy() )\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output)\n",
    "    plt.title('Model Prediction')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    # Calculate PSNR for each image pair in the batch\n",
    "    for i in range(outputs.shape[0]):  # Iterate over batch size\n",
    "        psnr_value = calculate_psnr(outputs[i].transpose(1, 2, 0), labels[i].transpose(1, 2, 0))  # Calculate PSNR for RGB images\n",
    "        psnr_values.append(psnr_value)\n",
    "\n",
    "# Compute average PSNR over all images in the dataset\n",
    "average_psnr = np.mean(psnr_values)\n",
    "print(f\"Average PSNR: {average_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model using test images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
